{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#one-hot encoding method\n",
    "df = pd.read_csv(\n",
    "        'http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data',\n",
    "        header=None)\n",
    "\n",
    "df.columns = ['buying', 'maint', 'doors', 'persons', 'trunk', 'safety', 'class']\n",
    "\n",
    "# One hot encode caegorical attributes\n",
    "df_dummies = pd.get_dummies(df.drop('class', axis=1))\n",
    "\n",
    "# Convert class to integers\n",
    "y = df['class'].replace(to_replace=['unacc', 'acc', 'good', 'vgood'],\n",
    "                        value=range(4)).values\n",
    "X = df_dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just start with the vectorized version and minibatch\n",
    "class LeagueIsLife():\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 l2_C=0.0, epochs=500, eta=0.001, random_state=None, alpha=0.0, decrease_const=0.0, \n",
    "                 shuffle=True, minibatches=1, f_cost='quadratic', f_layer=\"sigmoid\", tanh_a=0.0):\n",
    "        \n",
    "        #original initializer\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = l2_C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.f_cost = f_cost\n",
    "        self.f_layer = f_layer\n",
    "        self.tanh_a = tanh_a\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        if self.f_layer == 'relu':\n",
    "            init_bound = np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "            W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "            \n",
    "            init_bound = np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "            W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "            \n",
    "            init_bound = np.sqrt(2. / (self.n_output_ + self.n_hidden + 1))\n",
    "            W3 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1))\n",
    "            \n",
    "        elif self.f_layer =='tanh':\n",
    "            W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "            W1 = np.random.uniform(-6. / np.sqrt(W1_num_elems), 6. / np.sqrt(W1_num_elems), size=W1_num_elems)\n",
    "            W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "            \n",
    "            W2_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "            W2 = np.random.uniform(-6. / np.sqrt(W2_num_elems), 6. / np.sqrt(W2_num_elems), size=W2_num_elems)\n",
    "            W2 = W2.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "                \n",
    "            W3_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "            W3 = np.random.uniform(-6. / np.sqrt(W3_num_elems), 6. / np.sqrt(W3_num_elems), size=W3_num_elems)\n",
    "            W3 = W3.reshape(self.n_output_, self.n_hidden + 1)\n",
    "            \n",
    "        else:\n",
    "            W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "            W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "            W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "            W2_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "            W2 = np.random.uniform(-1.0, 1.0,size=W2_num_elems)\n",
    "            W2 = W2.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "            W3_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "            W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "            W3 = W3.reshape(self.n_output_, self.n_hidden + 1)\n",
    "            \n",
    "        return W1, W2, W3\n",
    "        \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu(Z):\n",
    "        return np.maximum(0,Z.copy())\n",
    "    \n",
    "    #@staticmethod\n",
    "    def _tanh(self, Z):\n",
    "        return np.tanh(Z) + (self.tanh_a * Z)\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        Z1 = W1 @ A1.T\n",
    "              \n",
    "        A2 = self._add_bias_unit(A1, how='row')\n",
    "        Z2 = W2 @ A2.T\n",
    "        \n",
    "        if self.f_layer == 'relu':\n",
    "            A3 = self._relu(Z2)\n",
    "        elif self.f_layer == 'tanh':\n",
    "            A3 = self._tanh(Z2)\n",
    "        else:\n",
    "            A3 = self._sigmoid(Z2) \n",
    "            \n",
    "        A3 = self._add_bias_unit(A3,how='row')\n",
    "        Z3 = W3 @ A3\n",
    "        \n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "        \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3 = self._initialize_weights()\n",
    "\n",
    "        delta_W1_prev = np.zeros(self.W1.shape)\n",
    "        delta_W2_prev = np.zeros(self.W2.shape)\n",
    "        delta_W3_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                        self.W3)\n",
    "                \n",
    "                cost = self._cost(A4,Y_enc[:,idx],self.W1,self.W2,self.W3)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2, grad3 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2,\n",
    "                                                  Z3=Z3,A4=A4,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2,W3=self.W3)\n",
    "\n",
    "                delta_W1, delta_W2, delta_W3 = self.eta * grad1, self.eta * grad2, self.eta * grad3\n",
    "                self.W1 -= (delta_W1 + (self.alpha * delta_W1_prev))\n",
    "                self.W2 -= (delta_W2 + (self.alpha * delta_W2_prev))\n",
    "                self.W3 -= (delta_W3 + (self.alpha * delta_W3_prev))\n",
    "\n",
    "                delta_W1_prev, delta_W2_prev, delta_W3_prev = delta_W1, delta_W2, delta_W3\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"alpha\": self.alpha, \"f_cost\": self.f_cost, \"f_layer\": self.f_layer, \"l2_C\": self.l2_C, \n",
    "               \"eta\": self.eta, \"decrease_const\": self.decrease_const, \"epochs\": self.epochs}\n",
    "    \n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        # Cross entropy choice\n",
    "        if self.f_cost == 'cross':\n",
    "            '''Get the objective function value'''\n",
    "            cost = np.mean((self._add_bias_unit(Y_enc)-A4)**2)\n",
    "            L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "            return cost + L2_term\n",
    "        # Quadradic (default) choice\n",
    "        else:\n",
    "            '''Get the objective function value'''\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "            L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "            return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3,A4, Z1, Z2,Z3, Y_enc, W1, W2,W3):\n",
    "                \n",
    "        # Cross entrophy choice\n",
    "        if self.f_cost == 'cross':\n",
    "            sigma4 = (A4-Y_enc)\n",
    "        else:\n",
    "            sigma4 = -2*(Y_enc-A4)*A4*(1-A4)\n",
    "        \n",
    "        # Relu choice\n",
    "        if self.f_layer == 'relu':\n",
    "            sigma3 = (W3.T @ sigma4) \n",
    "            Z3_with_bias = self._add_bias_unit(Z3,how='row')\n",
    "            sigma3[Z3_with_bias<=0] = 0\n",
    "        else:\n",
    "            sigma3 = (W3.T @ sigma4)*A3*(1-A3)\n",
    "        \n",
    "        # Relu choice\n",
    "        if self.f_layer == 'relu':\n",
    "            sigma2 = (W2.T @ sigma3) \n",
    "            Z1_with_bias = self._add_bias_unit(Z1,how='row')\n",
    "            sigma2[Z1_with_bias<=0] = 0\n",
    "        else:\n",
    "            sigma2 = (W2.T @ sigma3)*A2*(1-A2)\n",
    "        \n",
    "        grad1 = sigma2[1:,:] @ A1\n",
    "        grad2 = sigma3 @ A2.T\n",
    "        grad3 = sigma4 @ A3\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l3_C\n",
    "        return grad1, grad2, grad3\n",
    "    \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:,1:] ** 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            self.__setattr__(parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,31) and (30,1729) not aligned: 31 (dim 1) != 30 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6e6ac7efc5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a = LeagueIsLife(f_layer='relu', f_cost='cross', alpha=0, decrease_const=1e-09, \n\u001b[1;32m      2\u001b[0m            epochs=500, l2_C=0.01)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-7eae1392b30d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress)\u001b[0m\n\u001b[1;32m    132\u001b[0m                                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                                                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                                                         self.W3)\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7eae1392b30d>\u001b[0m in \u001b[0;36m_feedforward\u001b[0;34m(self, X, W1, W2, W3)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mA3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mZ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW3\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mA3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mA4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,31) and (30,1729) not aligned: 31 (dim 1) != 30 (dim 0)"
     ]
    }
   ],
   "source": [
    "a = LeagueIsLife(f_layer='relu', f_cost='cross', alpha=0, decrease_const=1e-09, \n",
    "           epochs=500, l2_C=0.01)\n",
    "a.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
